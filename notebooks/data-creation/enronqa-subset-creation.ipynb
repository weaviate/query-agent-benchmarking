{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a Subset from the EnronQA Dataset\n",
    "\n",
    "EnronQA contains emails from different people.\n",
    "\n",
    "This notebook creates a separate dataset only consisting of dasovich-j's emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4891\n",
      "11782\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import tiktoken\n",
    "\n",
    "ds = load_dataset(\"MichaelR207/enron_qa_0922\")\n",
    "\n",
    "filtered_ds = DatasetDict({\n",
    "    split: dataset.filter(lambda example: example[\"user\"] == \"dasovich-j\")\n",
    "    for split, dataset in ds.items()\n",
    "})\n",
    "\n",
    "def add_id(example, idx):\n",
    "    return {\"id\": idx}\n",
    "\n",
    "# add the index‐based id to each row\n",
    "filtered_ds = filtered_ds.map(add_id, with_indices=True)\n",
    "\n",
    "email_list = [\n",
    "    {\"dataset_id\": row[\"id\"], \"email_body\": row[\"email\"]}\n",
    "    for row in filtered_ds[\"train\"]\n",
    "]\n",
    "\n",
    "# only include questions marked retrieval‑only (include_email == 0)\n",
    "question_list = []\n",
    "for row in filtered_ds[\"train\"]:\n",
    "    row_id = row[\"id\"]\n",
    "    for flag, question in zip(row[\"include_email\"], row[\"rephrased_questions\"]):\n",
    "        if flag == 0:\n",
    "            question_list.append({\"dataset_id\": row_id, \"question\": question})\n",
    "\n",
    "# remove emails with longer than 2048 tokens\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "email_token_counts = []\n",
    "emails_to_keep = []\n",
    "questions_to_keep = []\n",
    "removed_email_ids = []\n",
    "\n",
    "for i, email in enumerate(email_list):\n",
    "    token_count = len(encoding.encode(email[\"email_body\"]))\n",
    "    email_token_counts.append(token_count)\n",
    "    if token_count <= 2048:\n",
    "        emails_to_keep.append(email)\n",
    "    else:\n",
    "        removed_email_ids.append(email[\"dataset_id\"])\n",
    "\n",
    "# Filter question_list to remove questions associated with removed emails\n",
    "for question in question_list:\n",
    "    if question[\"dataset_id\"] not in removed_email_ids:\n",
    "        questions_to_keep.append(question)\n",
    "\n",
    "email_list = emails_to_keep\n",
    "questions_list = questions_to_keep\n",
    "\n",
    "print(len(email_list))\n",
    "print(len(questions_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4891 emails to data/enron_emails.json\n",
      "Saved 11782 questions to data/enron_questions.json\n",
      "Saved 11782 question-email references to data/enron_qa_reference.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save emails to a separate JSON file\n",
    "emails_output_file = \"enron_emails.json\"\n",
    "with open(emails_output_file, \"w\") as f:\n",
    "    json.dump(email_list, f, indent=2)\n",
    "\n",
    "# Save questions to a separate JSON file\n",
    "questions_output_file = \"enron_questions.json\"\n",
    "with open(questions_output_file, \"w\") as f:\n",
    "    json.dump(questions_list, f, indent=2)\n",
    "\n",
    "# Create a reference file that maps questions to their corresponding emails\n",
    "reference_data = []\n",
    "email_dict = {email[\"dataset_id\"]: email[\"email_body\"] for email in email_list}\n",
    "\n",
    "for question in questions_list:\n",
    "    dataset_id = question[\"dataset_id\"]\n",
    "    if dataset_id in email_dict:\n",
    "        reference_data.append({\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"question_id\": id(question)  # Using object id as a unique identifier\n",
    "        })\n",
    "\n",
    "reference_output_file = \"enron_qa_reference.json\"\n",
    "with open(reference_output_file, \"w\") as f:\n",
    "    json.dump(reference_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(email_list)} emails to {emails_output_file}\")\n",
    "print(f\"Saved {len(questions_list)} questions to {questions_output_file}\")\n",
    "print(f\"Saved {len(reference_data)} question-email references to {reference_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
