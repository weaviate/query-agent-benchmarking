{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92275cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06a45dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='Hello! ...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello! ðŸ˜Š How can I help you today?']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dspy setup\n",
    "lm = dspy.LM(\n",
    "    \"openai/gpt-4.1\",\n",
    "    cache=False,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm, track_usage=True)\n",
    "\n",
    "lm(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5954c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='HyDE stands for Hydrogen Evolution (HyDE) and is a software tool designed for the automated identification and assessment of hydrogen bonds and other weak interactions in macromolecular structures, particularly those determined by X-ray crystallography or cryo-EM. Originally developed and used in structural biology and bioinformatics, HyDE analyzes protein-ligand interactions, emphasizing hydrogen bonding, hydrophobic contacts, and other intermolecular forces. The results from HyDE can help in drug design and understanding protein function.\\n\\nNote: In different scientific or technological contexts, \"HyDE\" could refer to other things. For example, in chemistry and drug discovery, HyDE may refer to a \"Hydrogen bond and Dehydration scoring function\" used for protein-ligand interaction assessment. Always consider the subject area to interpret \"HyDE\" correctly.'\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GenerateAnswerFromParameters(dspy.Signature):\n",
    "    \"\"\"Answer the question as well as you can.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question to answer.\")\n",
    "    answer: str = dspy.OutputField(description=\"The answer to the question.\")\n",
    "\n",
    "qa_system = dspy.Predict(GenerateAnswerFromParameters)\n",
    "\n",
    "qa_system(question=\"What is HyDE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df0c2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RAG systems\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from typing import Any, Literal\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the context and answer the question.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question to answer.\")\n",
    "    context: list[str] | list[dspy.Image] = dspy.InputField(description=\"The context to use to answer the question.\")\n",
    "    answer: str = dspy.OutputField(description=\"The answer to the question.\")\n",
    "\n",
    "class RAGSystem(dspy.Module):\n",
    "    def __init__(self, collection: Any, images_or_text: Literal[\"images\", \"text\"], k: int = 5):\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "        self.collection = collection\n",
    "        self.images_or_text = images_or_text\n",
    "        self.k = k\n",
    "    def _get_objects(self, question: str) -> list[str] | list[dspy.Image]:\n",
    "        if self.images_or_text == \"images\":\n",
    "            response = self.collection.query.near_text(\n",
    "                query=question,\n",
    "                return_properties=[\"base64_str\"],\n",
    "                limit=self.k\n",
    "            )\n",
    "            objects = []\n",
    "            for o in response.objects:\n",
    "                b64_str = o.properties[\"base64_str\"]\n",
    "                decoded_b64 = base64.b64decode(b64_str)\n",
    "                pil_image = Image.open(BytesIO(decoded_b64))\n",
    "                objects.append(dspy.Image(pil_image))\n",
    "            return objects\n",
    "        elif self.images_or_text == \"text\":\n",
    "            response = self.collection.query.hybrid(\n",
    "                query=question,\n",
    "                return_properties=[\"content\"],\n",
    "                limit=self.k\n",
    "            )\n",
    "            objects = []\n",
    "            for o in response.objects:\n",
    "                objects.append(o.properties[\"content\"])\n",
    "            return objects\n",
    "        \n",
    "    def _fetch_oracle_context(\n",
    "        self,\n",
    "        oracle_context_id: str, \n",
    "    ) -> str | dspy.Image:\n",
    "        if self.images_or_text == \"images\":\n",
    "            response = self.collection.query.fetch_objects(\n",
    "                filters=Filter.by_property(\"dataset_id\").like(oracle_context_id),\n",
    "                return_properties=[\"base64_str\"]\n",
    "            )\n",
    "            b64_str = response.objects[0].properties[\"base64_str\"]\n",
    "            decoded_b64 = base64.b64decode(b64_str)\n",
    "            pil_image = Image.open(BytesIO(decoded_b64))\n",
    "            return dspy.Image(pil_image)\n",
    "            \n",
    "        elif self.images_or_text == \"text\":\n",
    "            response = self.collection.query.fetch_objects(\n",
    "                filters=Filter.by_property(\"dataset_id\").like(oracle_context_id),\n",
    "                return_properties=[\"content\"]\n",
    "            )\n",
    "            return response.objects[0].properties[\"content\"]\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        question: str, \n",
    "        oracle_context_id: str = None\n",
    "    ) -> str:\n",
    "        if oracle_context_id is None:\n",
    "            context = self._get_objects(question)\n",
    "        else:\n",
    "            context = self._fetch_oracle_context(oracle_context_id)\n",
    "        return self.generate_answer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68a3c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:302: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "/var/folders/41/8dp_379x15d8zz4ppsjthdw40000gn/T/ipykernel_96876/304557697.py:8: ResourceWarning: unclosed <ssl.SSLSocket fd=1123, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('10.0.0.233', 55535), raddr=('3.78.128.217', 443)>\n",
      "  rag_system = RAGSystem(collection, \"images\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='HyDE stands for Hypothetical Document Embeddings. It is a novel approach for building effective dense retrievers in information retrieval without needing any labeled data for relevance. Instead of relying on annotated examples of which documents are relevant to which queries, HyDE uses large language models (LLMs) like InstructGPT to generate \"hypothetical\" answers to a queryâ€”i.e., a plausible document that would answer the queryâ€”without requiring this generated document to be factually correct or real.\\n\\nThe dense retriever then encodes these generated hypothetical documents using an unsupervised contrastive encoder (like Contriever or mContriever), and compares them to real documents encoded in the same way. This similarity search retrieves the most relevant real documents in response to the query. \\n\\nHyDE has two main steps:\\n1. For a given query, prompt the LLM to generate a hypothetical document answering the question.\\n2. Use an unsupervised retriever to encode and compare this generated document to the corpus, retrieving the most similar real documents.\\n\\nHyDE does not require any fine-tuning or adaptation of the models; it works \"out of the box.\" It separates dense retrieval into generative and retrieval stages, leveraging LLMs\\' ability to generate relevant textual examples and dense retrievers\\' ability to measure document similarity. HyDE is shown to outperform previous zero-shot and unsupervised methods, and can be especially effective at the early stages of building a retrieval system, before any relevance labels are available.'\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.environ[\"WEAVIATE_URL\"],\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(os.environ[\"WEAVIATE_API_KEY\"])\n",
    ")\n",
    "\n",
    "collection = weaviate_client.collections.get(\"IRPapersImages_Default\")\n",
    "\n",
    "rag_system = RAGSystem(collection, \"images\")\n",
    "\n",
    "rag_system(question=\"What is HyDE?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b06a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    score=False\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## sc...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    score=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# llm as judge\n",
    "class AssessAlignmentScore(dspy.Signature):\n",
    "    \"\"\"You are an expert grader assessing if a system's answer is semantically aligned with the correct answer.\n",
    "    Only return True if the system answer has essentially the same meaning as the correct answer.\n",
    "    If the system answer misses key aspects or meaning, return False.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question asked.\")\n",
    "    system_answer: str = dspy.InputField(description=\"The answer generated by the system.\")\n",
    "    correct_answer: str = dspy.InputField(description=\"The reference answer containing the correct and complete information.\")\n",
    "    score: bool = dspy.OutputField(description=\"True if system_answer is equivalent in meaning to correct_answer, otherwise False.\")\n",
    "\n",
    "judge = dspy.Predict(AssessAlignmentScore)\n",
    "\n",
    "test_question = \"What is HyDE?\"\n",
    "correct_answer = \"HyDE stands for Hypothetical Document Embeddings, a technique for improving retrieval in AI systems by generating hypothetical answers and using their embeddings.\"\n",
    "\n",
    "# System answer missing key aspect (embeddings)\n",
    "incorrect_answer = \"HyDE is a technique for improving retrieval in AI systems by generating hypothetical answers.\"\n",
    "# System answer rewords but covers all key ideas\n",
    "acceptable_answer = \"Hypothetical Document Embeddings (HyDE) is a method to help AI retrieval by creating hypothetical documents as sample answers and using their vector representations.\"\n",
    "\n",
    "response = judge(question=test_question, system_answer=incorrect_answer, correct_answer=correct_answer)\n",
    "print(response)\n",
    "response = judge(question=test_question, system_answer=acceptable_answer, correct_answer=correct_answer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81134a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai/gpt-4.1': {'completion_tokens': 12,\n",
       "  'prompt_tokens': 349,\n",
       "  'total_tokens': 361,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0,\n",
       "   'text_tokens': None},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0,\n",
       "   'cached_tokens': 0,\n",
       "   'text_tokens': None,\n",
       "   'image_tokens': None}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_lm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ded2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "queries = load_dataset(\"weaviate/irpapers-queries\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alignment_scores = np.array([], dtype=np.float32)\n",
    "input_tokens = np.array([], dtype=np.float32)\n",
    "output_tokens = np.array([], dtype=np.float32)\n",
    "\n",
    "K = 3\n",
    "\n",
    "rag_system = RAGSystem(collection, \"images\", k=5)\n",
    "\n",
    "start = time.time()\n",
    "for idx, query in enumerate(queries):\n",
    "    test_query, ground_truth_answer, oracle_context_id = query[\"question\"], query[\"answer\"], str(query[\"dataset_id\"])\n",
    "    qa_system_response = rag_system(\n",
    "        question=test_query,\n",
    "    )\n",
    "    usage_dict = qa_system_response.get_lm_usage()[\"openai/gpt-4.1\"]\n",
    "    input_tokens = np.append(input_tokens, usage_dict[\"prompt_tokens\"])\n",
    "    output_tokens = np.append(output_tokens, usage_dict[\"completion_tokens\"])\n",
    "\n",
    "    ensemble_votes = 0\n",
    "    for judge_predictions in range(K):\n",
    "        lm_judge_response = judge(\n",
    "            question=test_query,\n",
    "            system_answer=qa_system_response.answer,\n",
    "            correct_answer=ground_truth_answer\n",
    "        )\n",
    "        if lm_judge_response.score:\n",
    "            ensemble_votes += 1\n",
    "    if ensemble_votes >= K / 2:\n",
    "        alignment_scores = np.append(alignment_scores, 1)\n",
    "    else:\n",
    "        alignment_scores = np.append(alignment_scores, 0)\n",
    "\n",
    "    if idx % 5 == 4:\n",
    "        print(f\"Processed {idx+1} queries in {time.time() - start} seconds...\")\n",
    "        print(\"Alignment score running mean:\", alignment_scores.mean())\n",
    "        print(\"Input tokens running mean:\", input_tokens.mean())\n",
    "        print(\"Output tokens running mean:\", output_tokens.mean())\n",
    "        \n",
    "print(alignment_scores.mean())\n",
    "print(input_tokens.mean())\n",
    "print(output_tokens.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1713851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6923076923076923\n",
      "5199.596153846154\n",
      "178.1153846153846\n"
     ]
    }
   ],
   "source": [
    "alignment_scores = np.array(alignment_scores)\n",
    "input_tokens = np.array(input_tokens)\n",
    "output_tokens = np.array(output_tokens)\n",
    "\n",
    "print(alignment_scores.mean())\n",
    "print(input_tokens.mean())\n",
    "print(output_tokens.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
