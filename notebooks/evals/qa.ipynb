{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92275cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a45dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='Hello! H...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello! How can I help you today? ðŸ˜Š']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dspy setup\n",
    "lm = dspy.LM(\n",
    "    \"openai/gpt-4.1\",\n",
    "    cache=False,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm, track_usage=True)\n",
    "\n",
    "lm(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5954c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='HyDE stands for Hydrogen Evolution (HyDE) and is a software tool designed for the automated identification and assessment of hydrogen bonds and other weak interactions in macromolecular structures, particularly those determined by X-ray crystallography or cryo-EM. Originally developed and used in structural biology and bioinformatics, HyDE analyzes protein-ligand interactions, emphasizing hydrogen bonding, hydrophobic contacts, and other intermolecular forces. The results from HyDE can help in drug design and understanding protein function.\\n\\nNote: In different scientific or technological contexts, \"HyDE\" could refer to other things. For example, in chemistry and drug discovery, HyDE may refer to a \"Hydrogen bond and Dehydration scoring function\" used for protein-ligand interaction assessment. Always consider the subject area to interpret \"HyDE\" correctly.'\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GenerateAnswerFromParameters(dspy.Signature):\n",
    "    \"\"\"Answer the question as well as you can.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question to answer.\")\n",
    "    answer: str = dspy.OutputField(description=\"The answer to the question.\")\n",
    "\n",
    "qa_system = dspy.Predict(GenerateAnswerFromParameters)\n",
    "\n",
    "qa_system(question=\"What is HyDE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RAG systems\n",
    "from typing import Any, Literal\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Assess the context and answer the question.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question to answer.\")\n",
    "    context: list[str] | list[dspy.Image] = dspy.InputField(description=\"The context to use to answer the question.\")\n",
    "    answer: str = dspy.OutputField(description=\"The answer to the question.\")\n",
    "\n",
    "class RAGSystem(dspy.Module):\n",
    "    def __init__(self, collection: Any, images_or_text: Literal[\"image\", \"text\"], k: int = 5):\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "        self.collection = collection\n",
    "        self.images_or_text = images_or_text\n",
    "        self.k = k\n",
    "    def _get_objects(self, question: str) -> list[str] | list[dspy.Image]:\n",
    "        if self.images_or_text == \"image\":\n",
    "            response = self.collection.query.hybrid(\n",
    "                query=question,\n",
    "                return_properties=[\"base64_str\"],\n",
    "                limit=self.k\n",
    "            )\n",
    "            objects = []\n",
    "            for o in response.objects:\n",
    "                objects.append(o.properties[\"base64_str\"])\n",
    "            # do PIL magic and convert these to dspy.Image objects\n",
    "            return objects\n",
    "        elif self.images_or_text == \"text\":\n",
    "            response = self.collection.query.hybrid(\n",
    "                query=question,\n",
    "                return_properties=[\"content\"],\n",
    "                limit=self.k\n",
    "            )\n",
    "            objects = []\n",
    "            for o in response.objects:\n",
    "                objects.append(o.properties[\"content\"])\n",
    "            return objects\n",
    "        \n",
    "    def _fetch_oracle_context(\n",
    "        self,\n",
    "        oracle_context_id: str, \n",
    "    ) -> str | dspy.Image:\n",
    "        if self.images_or_text == \"image\":\n",
    "            response = self.collection.query.fetch_objects(\n",
    "                filters=Filter.by_property(\"dataset_id\").like(oracle_context_id),\n",
    "                return_properties=[\"base64_str\"]\n",
    "            )\n",
    "            return response.objects[0].properties[\"base64_str\"]\n",
    "            \n",
    "        elif self.images_or_text == \"text\":\n",
    "            response = self.collection.query.fetch_objects(\n",
    "                filters=Filter.by_property(\"dataset_id\").like(oracle_context_id),\n",
    "                return_properties=[\"content\"]\n",
    "            )\n",
    "            return response.objects[0].properties[\"content\"]\n",
    "\n",
    "    def __call__(\n",
    "        self, \n",
    "        question: str, \n",
    "        oracle_context_id: str = None\n",
    "    ) -> str:\n",
    "        if oracle_context_id is None:\n",
    "            context = self._get_objects(question)\n",
    "        else:\n",
    "            context = self._fetch_oracle_context(oracle_context_id)\n",
    "        return self.generate_answer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a3c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='HyDE stands for \"Hypothetical Document Embeddings.\" It is a method designed to build effective dense information retrievers in a completely unsupervised way, without relying on relevance labels or supervised fine-tuning. The core idea is to combine two types of models:\\n\\n1. **A generative, instruction-following language model (e.g., InstructGPT, GPT-3):** For each search query, the language model is tasked with generating a hypothetical document that would answer the query â€” even if it\\'s not a real document and may include imaginary or hallucinated content. The goal is for this generated text to closely resemble something relevant to the query.\\n\\n2. **A contrastive text encoder (e.g., Contriever, mContriever):** This encoder then converts the hypothetical document into a dense vector embedding. This embedding is comparedâ€”using inner product similarityâ€”to the embeddings of all real documents in the search corpus.\\n\\nThe top-ranked (most similar) real documents are retrieved and presented as search results.\\n\\n**Key features of HyDE:**\\n- **No Supervision Needed:** It requires no labeled relevance data or supervised training.\\n- **Works \"Out of the Box\":** Both the generative and encoding models are used as-is, without additional fine-tuning.\\n- **Flexible and General-purpose:** It works well for various search tasks, including those in different languages and low-resource domains.\\n- **Performance:** Experiments show that HyDE often outperforms previous unsupervised dense retrievers and is competitive with some fine-tuned models.\\n- **Intended Use:** HyDE is especially useful in scenarios where no relevance data is available â€” for instance, the early stages of search system deployment or for handling new, unseen types of queries.\\n\\nThe name HyDE refers to the process of generating a \"hypothetical\" document for each query, which is then embedded and used as a proxy to find relevant documents via dense retrieval.\\n\\n**How It Works (Simplified):**\\n1. Input query â†’ LLM writes a hypothetical document that answers the query.\\n2. That document is encoded into a vector.\\n3. The vector is used to search for the most similar real documents by comparing vectors.\\n4. The most similar (according to inner product similarity) documents are retrieved as the answer set.\\n\\nHyDE thus leverages advances in generative LLMs and contrastive encoders to sidestep the need for relevance labels, offering strong zero-shot retrieval performance.'\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.environ[\"WEAVIATE_URL\"],\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(os.environ[\"WEAVIATE_API_KEY\"])\n",
    ")\n",
    "\n",
    "collection = weaviate_client.collections.get(\"IRPapersText_Default\")\n",
    "\n",
    "rag_system = RAGSystem(collection, \"text\")\n",
    "\n",
    "rag_system(question=\"What is HyDE?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f1b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='GPT-3.5 generated query variants achieved up to 71.1% overlap in document pooling at depth 100 compared to human-generated variants in the UQV100 test collection.'\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question, oracle_context_id = queries[0][\"question\"], str(queries[0][\"dataset_id\"])\n",
    "\n",
    "rag_system(question=question, oracle_context_id=oracle_context_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b06a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    score=False\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## sc...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    score=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# llm as judge\n",
    "class AssessAlignmentScore(dspy.Signature):\n",
    "    \"\"\"You are an expert grader assessing if a system's answer is semantically aligned with the correct answer.\n",
    "    Only return True if the system answer has essentially the same meaning as the correct answer.\n",
    "    If the system answer misses key aspects or meaning, return False.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question asked.\")\n",
    "    system_answer: str = dspy.InputField(description=\"The answer generated by the system.\")\n",
    "    correct_answer: str = dspy.InputField(description=\"The reference answer containing the correct and complete information.\")\n",
    "    score: bool = dspy.OutputField(description=\"True if system_answer is equivalent in meaning to correct_answer, otherwise False.\")\n",
    "\n",
    "judge = dspy.Predict(AssessAlignmentScore)\n",
    "\n",
    "test_question = \"What is HyDE?\"\n",
    "correct_answer = \"HyDE stands for Hypothetical Document Embeddings, a technique for improving retrieval in AI systems by generating hypothetical answers and using their embeddings.\"\n",
    "\n",
    "# System answer missing key aspect (embeddings)\n",
    "incorrect_answer = \"HyDE is a technique for improving retrieval in AI systems by generating hypothetical answers.\"\n",
    "# System answer rewords but covers all key ideas\n",
    "acceptable_answer = \"Hypothetical Document Embeddings (HyDE) is a method to help AI retrieval by creating hypothetical documents as sample answers and using their vector representations.\"\n",
    "\n",
    "response = judge(question=test_question, system_answer=incorrect_answer, correct_answer=correct_answer)\n",
    "print(response)\n",
    "response = judge(question=test_question, system_answer=acceptable_answer, correct_answer=correct_answer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81134a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai/gpt-4.1': {'completion_tokens': 12,\n",
       "  'prompt_tokens': 349,\n",
       "  'total_tokens': 361,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0,\n",
       "   'text_tokens': None},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0,\n",
       "   'cached_tokens': 0,\n",
       "   'text_tokens': None,\n",
       "   'image_tokens': None}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_lm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ded2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from datasets import load_dataset\n",
    "\n",
    "queries = load_dataset(\"weaviate/irpapers-queries\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34d70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## an...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## sc...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_2\n",
      "1_3\n",
      "1_4\n",
      "Processed 5 queries in 25.57102084159851 seconds...\n",
      "1_5\n",
      "2_1\n",
      "2_2\n",
      "2_3\n"
     ]
    }
   ],
   "source": [
    "alignment_scores, input_tokens, output_tokens = [], [], []\n",
    "\n",
    "K = 3\n",
    "\n",
    "start = time.time()\n",
    "for idx, query in enumerate(queries):\n",
    "    if idx % 5 == 4:\n",
    "        print(f\"Processed {idx+1} queries in {time.time() - start} seconds...\")\n",
    "    test_query, ground_truth_answer, oracle_context_id = query[\"question\"], query[\"answer\"], str(query[\"dataset_id\"])\n",
    "    qa_system_response = rag_system(\n",
    "        question=test_query,\n",
    "        oracle_context_id=oracle_context_id\n",
    "    )\n",
    "    usage_dict = qa_system_response.get_lm_usage()[\"openai/gpt-4.1\"]\n",
    "    input_tokens.append(usage_dict[\"prompt_tokens\"])\n",
    "    output_tokens.append(usage_dict[\"completion_tokens\"])\n",
    "\n",
    "    ensemble_votes = 0\n",
    "    for judge_predictions in range(K):\n",
    "        lm_judge_response = judge(\n",
    "            question=test_query,\n",
    "            system_answer=qa_system_response.answer,\n",
    "            correct_answer=ground_truth_answer\n",
    "        )\n",
    "        if lm_judge_response.score:\n",
    "            ensemble_votes += 1\n",
    "    if ensemble_votes >= K / 2:\n",
    "        alignment_scores.append(1)\n",
    "    else:\n",
    "        alignment_scores.append(0)\n",
    "\n",
    "alignment_scores = np.array(alignment_scores)\n",
    "input_tokens = np.array(input_tokens)\n",
    "output_tokens = np.array(output_tokens)\n",
    "\n",
    "print(alignment_scores.mean())\n",
    "print(input_tokens.mean())\n",
    "print(output_tokens.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1713851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17647058823529413\n",
      "170.28846153846155\n",
      "123.32692307692308\n"
     ]
    }
   ],
   "source": [
    "alignment_scores = np.array(alignment_scores)\n",
    "input_tokens = np.array(input_tokens)\n",
    "output_tokens = np.array(output_tokens)\n",
    "\n",
    "print(alignment_scores.mean())\n",
    "print(input_tokens.mean())\n",
    "print(output_tokens.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
