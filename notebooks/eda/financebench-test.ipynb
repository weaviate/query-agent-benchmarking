{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3200fb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/weaviate/weaviate-python-client@multi2multivec-weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073e1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from base64 import b64encode, b64decode\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "import dspy\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9390ec",
   "metadata": {},
   "source": [
    "# Get the FinanceBench Dataset from HuggingFace and B64 encode each page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74060549",
   "metadata": {},
   "source": [
    "### Download PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cea4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"PatronusAI/financebench\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82806f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_pdf(args):\n",
    "    \"\"\"Download a single PDF from a URL\"\"\"\n",
    "    url, output_folder, index, doc_name = args\n",
    "    \n",
    "    try:\n",
    "        # Use doc_name for filename\n",
    "        if doc_name:\n",
    "            # Ensure it ends with .pdf\n",
    "            if not doc_name.lower().endswith('.pdf'):\n",
    "                filename = f\"{doc_name}.pdf\"\n",
    "            else:\n",
    "                filename = doc_name\n",
    "        else:\n",
    "            # Fallback if doc_name is missing\n",
    "            filename = f\"document_{index}.pdf\"\n",
    "        \n",
    "        output_path = Path(output_folder) / filename\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url, timeout=30, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"url\": url,\n",
    "            \"filename\": filename,\n",
    "            \"path\": str(output_path),\n",
    "            \"index\": index,\n",
    "            \"doc_name\": doc_name\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"url\": url,\n",
    "            \"error\": str(e),\n",
    "            \"index\": index,\n",
    "            \"doc_name\": doc_name\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cce905a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdfs_from_dataframe(df, url_column='doc_link', doc_name_column='doc_name',\n",
    "                                  output_folder='financebench-pdfs', max_workers=4, \n",
    "                                  delay_between_requests=0.1):\n",
    "    \"\"\"Download PDFs from URLs in a DataFrame column\"\"\"\n",
    "    \n",
    "    # Create output folder\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get URLs\n",
    "    if url_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{url_column}' not found in DataFrame\")\n",
    "    \n",
    "    if doc_name_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{doc_name_column}' not found in DataFrame\")\n",
    "    \n",
    "    urls = df[url_column].tolist()\n",
    "    doc_names = df[doc_name_column].tolist()\n",
    "    total_urls = len(urls)\n",
    "    \n",
    "    print(f\"Starting download of {total_urls} PDFs to '{output_folder}'...\")\n",
    "    print(f\"Using {max_workers} parallel workers\\n\")\n",
    "    \n",
    "    successful_downloads = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    # Download in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(download_single_pdf, (url, output_folder, idx, doc_name)): idx \n",
    "            for idx, (url, doc_name) in enumerate(zip(urls, doc_names))\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            \n",
    "            if result['success']:\n",
    "                successful_downloads.append(result)\n",
    "                print(f\"✓ [{result['index']+1}/{total_urls}] {result['filename']}\")\n",
    "            else:\n",
    "                failed_downloads.append(result)\n",
    "                print(f\"✗ [{result['index']+1}/{total_urls}] {result['url']}\")\n",
    "                print(f\"  Error: {result['error']}\")\n",
    "            \n",
    "            time.sleep(delay_between_requests)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Download Summary:\")\n",
    "    print(f\"  Total: {total_urls} | Successful: {len(successful_downloads)} | Failed: {len(failed_downloads)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(\"\\nFailed downloads:\")\n",
    "        for failure in failed_downloads:\n",
    "            print(f\"  - {failure['url']}: {failure['error']}\")\n",
    "    \n",
    "    return {\n",
    "        \"total\": total_urls,\n",
    "        \"successful\": successful_downloads,\n",
    "        \"failed\": failed_downloads,\n",
    "        \"success_count\": len(successful_downloads),\n",
    "        \"failure_count\": len(failed_downloads)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33154f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = download_pdfs_from_dataframe(\n",
    "    df=df,                              # Your DataFrame\n",
    "    url_column='doc_link',              # Column with URLs\n",
    "    doc_name_column='doc_name',         # Column with document names\n",
    "    output_folder='financebench-pdfs',  # Where to save PDFs\n",
    "    max_workers=4,                      # Parallel downloads\n",
    "    delay_between_requests=0.1          # Delay between requests (seconds)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Downloaded {results['success_count']} PDFs successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fa767",
   "metadata": {},
   "source": [
    "### Base64 Encodings per Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdd8603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPI = 300\n",
    "FORMAT = \"png\"\n",
    "PDF_FOLDER = \"financebench-pdfs\"\n",
    "OUTPUT_CSV = \"pdf_pages_base64.csv\"\n",
    "MAX_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c73482dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_pdf(args):\n",
    "    \"\"\"Process a single PDF file and return base64 encoded images for each page\"\"\"\n",
    "    pdf_file, dpi, fmt = args\n",
    "    \n",
    "    zoom = dpi / 72.0\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "    \n",
    "    start = time.time()\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"filename\": pdf_file.name}\n",
    "    \n",
    "    filename = pdf_file.stem\n",
    "    pages_data = []\n",
    "    total_image_bytes = 0\n",
    "    total_base64_bytes = 0\n",
    "    \n",
    "    for i, page in enumerate(doc, start=1):\n",
    "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "        buf = io.BytesIO()\n",
    "        img_bytes = pix.pil_tobytes(format=fmt.upper())\n",
    "        buf.write(img_bytes)\n",
    "        byte_data = buf.getvalue()\n",
    "        \n",
    "        # Get size before base64 encoding\n",
    "        image_size_bytes = len(byte_data)\n",
    "        \n",
    "        # Encode to base64\n",
    "        base64_str = b64encode(byte_data).decode(\"utf-8\")\n",
    "        \n",
    "        # Get size after base64 encoding\n",
    "        base64_size_bytes = len(base64_str.encode('utf-8'))\n",
    "        \n",
    "        # Track totals\n",
    "        total_image_bytes += image_size_bytes\n",
    "        total_base64_bytes += base64_size_bytes\n",
    "        \n",
    "        pages_data.append({\n",
    "            \"source_pdf\": pdf_file.name,\n",
    "            \"page_number\": i,\n",
    "            \"base64\": base64_str,\n",
    "            \"image_size_bytes\": image_size_bytes,\n",
    "            \"base64_size_bytes\": base64_size_bytes\n",
    "        })\n",
    "    \n",
    "    doc.close()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = (total_base64_bytes / total_image_bytes) if total_image_bytes > 0 else 0\n",
    "    \n",
    "    print(f\"✓ Processed {len(pages_data)} pages from {pdf_file.name} in {elapsed:.2f}s | \"\n",
    "          f\"Image: {total_image_bytes/1024/1024:.2f}MB → Base64: {total_base64_bytes/1024/1024:.2f}MB \"\n",
    "          f\"(ratio: {compression_ratio:.2f}x)\")\n",
    "    \n",
    "    return {\n",
    "        \"filename\": pdf_file.name,\n",
    "        \"pdf_stem\": filename,\n",
    "        \"pages\": pages_data,\n",
    "        \"num_pages\": len(pages_data),\n",
    "        \"time\": elapsed,\n",
    "        \"total_image_bytes\": total_image_bytes,\n",
    "        \"total_base64_bytes\": total_base64_bytes,\n",
    "        \"compression_ratio\": compression_ratio\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc297fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_to_base64(pdf_folder=PDF_FOLDER, dpi=DPI, fmt=FORMAT, \n",
    "                           max_workers=MAX_WORKERS, number_pdfs=None):\n",
    "    \"\"\"Process all PDFs in folder and convert each page to base64\"\"\"\n",
    "    \n",
    "    pdf_path = Path(pdf_folder)\n",
    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_folder}\")\n",
    "        return []\n",
    "    \n",
    "    total_pdfs = len(pdf_files)\n",
    "    \n",
    "    if number_pdfs is not None:\n",
    "        pdf_files = pdf_files[:number_pdfs]\n",
    "        print(f\"Processing {len(pdf_files)} of {total_pdfs} PDFs using {max_workers} workers...\")\n",
    "    else:\n",
    "        print(f\"Processing all {total_pdfs} PDFs using {max_workers} workers...\")\n",
    "    \n",
    "    print(f\"DPI: {dpi}, Format: {fmt}\\n\")\n",
    "    \n",
    "    failed_to_open = []\n",
    "    all_results = []\n",
    "    \n",
    "    # Process PDFs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_single_pdf, (pdf_file, dpi, fmt)): pdf_file \n",
    "            for pdf_file in pdf_files\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            \n",
    "            # Check if processing failed\n",
    "            if \"error\" in result and \"pages\" not in result:\n",
    "                failed_to_open.append(result[\"filename\"])\n",
    "                print(f\"✗ Could not open {result['filename']}: {result['error']}\")\n",
    "            else:\n",
    "                all_results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"  Total PDFs: {len(pdf_files)}\")\n",
    "    print(f\"  Successfully processed: {len(all_results)}\")\n",
    "    print(f\"  Failed: {len(failed_to_open)}\")\n",
    "    total_pages = sum(r['num_pages'] for r in all_results)\n",
    "    total_img_bytes = sum(r['total_image_bytes'] for r in all_results)\n",
    "    total_b64_bytes = sum(r['total_base64_bytes'] for r in all_results)\n",
    "    print(f\"  Total pages extracted: {total_pages}\")\n",
    "    print(f\"  Total image size: {total_img_bytes/1024/1024:.2f} MB\")\n",
    "    print(f\"  Total base64 size: {total_b64_bytes/1024/1024:.2f} MB\")\n",
    "    if total_img_bytes > 0:\n",
    "        print(f\"  Overall compression ratio: {total_b64_bytes/total_img_bytes:.2f}x\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if failed_to_open:\n",
    "        print(\"\\nFailed to open:\")\n",
    "        for fname in failed_to_open:\n",
    "            print(f\"  - {fname}\")\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "082794e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, output_file=OUTPUT_CSV):\n",
    "    \"\"\"Save the base64 encodings to a CSV file\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results to save\")\n",
    "        return\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['pdf_filename', 'pdf_stem', 'page_number', 'image_size_bytes', \n",
    "                      'base64_size_bytes', 'base64_encoding']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for pdf_result in results:\n",
    "            for page in pdf_result['pages']:\n",
    "                writer.writerow({\n",
    "                    'pdf_filename': pdf_result['filename'],\n",
    "                    'pdf_stem': pdf_result['pdf_stem'],\n",
    "                    'page_number': page['page_number'],\n",
    "                    'image_size_bytes': page['image_size_bytes'],\n",
    "                    'base64_size_bytes': page['base64_size_bytes'],\n",
    "                    'base64_encoding': page['base64']\n",
    "                })\n",
    "    \n",
    "    print(f\"\\n✓ Saved base64 encodings to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d4d682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all 74 PDFs using 4 workers...\n",
      "DPI: 300, Format: png\n",
      "\n",
      "✗ Could not open KRAFTHEINZ_2019_10K.pdf: cannot open broken document\n",
      "✓ Processed 181 pages from AMERICANWATERWORKS_2021_10K.pdf in 24.68s | Image: 77.74MB → Base64: 103.65MB (ratio: 1.33x)\n",
      "✓ Processed 179 pages from JPMORGAN_2021Q1_10Q.pdf in 29.63s | Image: 104.53MB → Base64: 139.37MB (ratio: 1.33x)\n",
      "✓ Processed 195 pages from AMCOR_2020_10K.pdf in 30.40s | Image: 85.88MB → Base64: 114.51MB (ratio: 1.33x)\n",
      "✓ Processed 220 pages from MGMRESORTS_2022_10K.pdf in 33.80s | Image: 115.43MB → Base64: 153.91MB (ratio: 1.33x)\n",
      "✓ Processed 92 pages from 3M_2023Q2_10Q.pdf in 13.40s | Image: 46.98MB → Base64: 62.63MB (ratio: 1.33x)\n",
      "✓ Processed 73 pages from NETFLIX_2017_10K.pdf in 10.74s | Image: 35.57MB → Base64: 47.43MB (ratio: 1.33x)\n",
      "✓ Processed 152 pages from AMERICANWATERWORKS_2020_10K.pdf in 24.51s | Image: 88.98MB → Base64: 118.64MB (ratio: 1.33x)\n",
      "✓ Processed 213 pages from CVSHEALTH_2022_10K.pdf in 34.51s | Image: 125.04MB → Base64: 166.71MB (ratio: 1.33x)\n",
      "✓ Processed 170 pages from WALMART_2020_10K.pdf in 26.69s | Image: 91.35MB → Base64: 121.80MB (ratio: 1.33x)\n",
      "✓ Processed 260 pages from AMERICANEXPRESS_2022_10K.pdf in 39.71s | Image: 130.05MB → Base64: 173.40MB (ratio: 1.33x)\n",
      "✓ Processed 183 pages from COCACOLA_2022_10K.pdf in 26.13s | Image: 87.68MB → Base64: 116.91MB (ratio: 1.33x)\n",
      "✓ Processed 75 pages from BESTBUY_2023_10K.pdf in 11.59s | Image: 39.40MB → Base64: 52.53MB (ratio: 1.33x)\n",
      "✓ Processed 121 pages from MICROSOFT_2016_10K.pdf in 18.87s | Image: 62.26MB → Base64: 83.01MB (ratio: 1.33x)\n",
      "✓ Processed 159 pages from CORNING_2022_10K.pdf in 23.54s | Image: 73.56MB → Base64: 98.07MB (ratio: 1.33x)\n",
      "✓ Processed 109 pages from BESTBUY_2017_10K.pdf in 16.70s | Image: 52.77MB → Base64: 70.37MB (ratio: 1.33x)\n",
      "✓ Processed 198 pages from ACTIVISIONBLIZZARD_2019_10K.pdf in 28.98s | Image: 88.81MB → Base64: 118.41MB (ratio: 1.33x)\n",
      "✗ Could not open AMD_2015_10K.pdf: cannot open broken document\n",
      "✓ Processed 104 pages from NIKE_2019_10K.pdf in 15.67s | Image: 49.86MB → Base64: 66.48MB (ratio: 1.33x)\n",
      "✓ Processed 156 pages from AMCOR_2023_10K.pdf in 28.23s | Image: 85.52MB → Base64: 114.02MB (ratio: 1.33x)\n",
      "✓ Processed 549 pages from PEPSICO_2021_10K.pdf in 77.53s | Image: 213.57MB → Base64: 284.76MB (ratio: 1.33x)\n",
      "✓ Processed 190 pages from BOEING_2022_10K.pdf in 28.99s | Image: 87.96MB → Base64: 117.28MB (ratio: 1.33x)\n",
      "✓ Processed 72 pages from NETFLIX_2015_10K.pdf in 11.06s | Image: 35.06MB → Base64: 46.75MB (ratio: 1.33x)\n",
      "✓ Processed 31 pages from FOOTLOCKER_2022_8K_dated_2022-08-19.pdf in 4.50s | Image: 12.87MB → Base64: 17.16MB (ratio: 1.33x)\n",
      "✓ Processed 83 pages from AMAZON_2019_10K.pdf in 13.32s | Image: 41.87MB → Base64: 55.83MB (ratio: 1.33x)\n",
      "✓ Processed 306 pages from AMERICANWATERWORKS_2022_10K.pdf in 50.15s | Image: 166.14MB → Base64: 221.52MB (ratio: 1.33x)\n",
      "✓ Processed 257 pages from AES_2022_10K.pdf in 44.86s | Image: 158.92MB → Base64: 211.90MB (ratio: 1.33x)\n",
      "✓ Processed 138 pages from CORNING_2020_10K.pdf in 22.68s | Image: 72.86MB → Base64: 97.15MB (ratio: 1.33x)\n",
      "✓ Processed 140 pages from GENERALMILLS_2019_10K.pdf in 21.67s | Image: 67.57MB → Base64: 90.09MB (ratio: 1.33x)\n",
      "✓ Processed 158 pages from BLOCK_2020_10K.pdf in 23.67s | Image: 78.25MB → Base64: 104.33MB (ratio: 1.33x)\n",
      "✓ Processed 121 pages from AMD_2022_10K.pdf in 16.51s | Image: 56.06MB → Base64: 74.74MB (ratio: 1.33x)\n",
      "✓ Processed 4 pages from FOOTLOCKER_2022_8K_dated-2022-05-20.pdf in 0.57s | Image: 1.29MB → Base64: 1.71MB (ratio: 1.33x)\n",
      "✓ Processed 183 pages from COCACOLA_2021_10K.pdf in 25.84s | Image: 91.04MB → Base64: 121.39MB (ratio: 1.33x)\n",
      "✓ Processed 125 pages from CORNING_2021_10K.pdf in 17.73s | Image: 59.69MB → Base64: 79.58MB (ratio: 1.33x)\n",
      "✓ Processed 116 pages from MICROSOFT_2023_10K.pdf in 15.39s | Image: 48.25MB → Base64: 64.33MB (ratio: 1.33x)\n",
      "✓ Processed 97 pages from NIKE_2018_10K.pdf in 14.75s | Image: 52.40MB → Base64: 69.86MB (ratio: 1.33x)\n",
      "✓ Processed 252 pages from 3M_2022_10K.pdf in 35.67s | Image: 128.93MB → Base64: 171.90MB (ratio: 1.33x)\n",
      "✓ Processed 150 pages from MGMRESORTS_2020_10K.pdf in 22.70s | Image: 87.47MB → Base64: 116.63MB (ratio: 1.33x)\n",
      "✓ Processed 9 pages from ULTABEAUTY_2023Q4_EARNINGS.pdf in 1.36s | Image: 4.52MB → Base64: 6.03MB (ratio: 1.33x)\n",
      "✓ Processed 503 pages from PEPSICO_2022_10K.pdf in 71.55s | Image: 215.64MB → Base64: 287.52MB (ratio: 1.33x)\n",
      "✓ Processed 197 pages from COCACOLA_2017_10K.pdf in 27.62s | Image: 94.14MB → Base64: 125.52MB (ratio: 1.33x)\n",
      "✓ Processed 121 pages from BLOCK_2016_10K.pdf in 17.31s | Image: 55.76MB → Base64: 74.34MB (ratio: 1.33x)\n",
      "✓ Processed 72 pages from Pfizer_2023Q2_10Q.pdf in 11.38s | Image: 42.38MB → Base64: 56.50MB (ratio: 1.33x)\n",
      "✓ Processed 172 pages from PEPSICO_2023_8K_dated-2023-05-30.pdf in 23.56s | Image: 72.03MB → Base64: 96.04MB (ratio: 1.33x)\n",
      "✓ Processed 57 pages from AMCOR_2023Q2_10Q.pdf in 9.09s | Image: 24.53MB → Base64: 32.70MB (ratio: 1.33x)\n",
      "✓ Processed 160 pages from 3M_2018_10K.pdf in 22.65s | Image: 79.19MB → Base64: 105.59MB (ratio: 1.33x)\n",
      "✓ Processed 76 pages from COSTCO_2021_10K.pdf in 11.55s | Image: 35.37MB → Base64: 47.16MB (ratio: 1.33x)\n",
      "✓ Processed 155 pages from WALMART_2019_10K.pdf in 21.64s | Image: 63.82MB → Base64: 85.09MB (ratio: 1.33x)\n",
      "✓ Processed 127 pages from GENERALMILLS_2022_10K.pdf in 17.06s | Image: 47.70MB → Base64: 63.61MB (ratio: 1.33x)\n",
      "✓ Processed 197 pages from JPMORGAN_2022Q2_10Q.pdf in 30.96s | Image: 110.66MB → Base64: 147.55MB (ratio: 1.33x)\n",
      "✓ Processed 130 pages from LOCKHEEDMARTIN_2022_10K.pdf in 19.74s | Image: 87.71MB → Base64: 116.95MB (ratio: 1.33x)\n",
      "✓ Processed 120 pages from VERIZON_2021_10K.pdf in 16.97s | Image: 77.37MB → Base64: 103.16MB (ratio: 1.33x)\n",
      "✓ Processed 5 pages from PEPSICO_2023_8K_dated-2023-05-05.pdf in 0.56s | Image: 1.52MB → Base64: 2.03MB (ratio: 1.33x)\n",
      "✓ Processed 106 pages from NIKE_2023_10K.pdf in 14.97s | Image: 58.49MB → Base64: 77.98MB (ratio: 1.33x)\n",
      "✓ Processed 136 pages from BOEING_2018_10K.pdf in 19.07s | Image: 61.78MB → Base64: 82.37MB (ratio: 1.33x)\n",
      "✓ Processed 105 pages from ULTABEAUTY_2023_10K.pdf in 13.80s | Image: 45.69MB → Base64: 60.92MB (ratio: 1.33x)\n",
      "✓ Processed 382 pages from JPMORGAN_2022_10K.pdf in 63.66s | Image: 257.93MB → Base64: 343.90MB (ratio: 1.33x)\n",
      "✓ Processed 132 pages from LOCKHEEDMARTIN_2020_10K.pdf in 19.47s | Image: 77.51MB → Base64: 103.34MB (ratio: 1.33x)\n",
      "✓ Processed 9 pages from AMCOR_2022_8K_dated-2022-07-01.pdf in 1.45s | Image: 4.43MB → Base64: 5.90MB (ratio: 1.33x)\n",
      "✓ Processed 127 pages from GENERALMILLS_2020_10K.pdf in 19.13s | Image: 77.92MB → Base64: 103.89MB (ratio: 1.33x)\n",
      "✓ Processed 304 pages from WALMART_2018_10K.pdf in 43.64s | Image: 157.63MB → Base64: 210.18MB (ratio: 1.33x)\n",
      "✓ Processed 62 pages from MGMRESORTS_2023Q2_10Q.pdf in 9.55s | Image: 29.53MB → Base64: 39.37MB (ratio: 1.33x)\n",
      "✓ Processed 107 pages from BESTBUY_2019_10K.pdf in 16.11s | Image: 53.67MB → Base64: 71.56MB (ratio: 1.33x)\n",
      "✓ Processed 14 pages from AMCOR_2023Q4_EARNINGS.pdf in 2.12s | Image: 8.12MB → Base64: 10.82MB (ratio: 1.33x)\n",
      "✓ Processed 109 pages from NIKE_2021_10K.pdf in 15.81s | Image: 51.08MB → Base64: 68.11MB (ratio: 1.33x)\n",
      "✓ Processed 134 pages from PAYPAL_2022_10K.pdf in 20.84s | Image: 75.17MB → Base64: 100.23MB (ratio: 1.33x)\n",
      "✓ Processed 85 pages from AMAZON_2017_10K.pdf in 12.63s | Image: 43.65MB → Base64: 58.19MB (ratio: 1.33x)\n",
      "✓ Processed 143 pages from PFIZER_2021_10K.pdf in 22.28s | Image: 83.74MB → Base64: 111.65MB (ratio: 1.33x)\n",
      "✓ Processed 217 pages from JPMORGAN_2023Q2_10Q.pdf in 34.89s | Image: 126.05MB → Base64: 168.06MB (ratio: 1.33x)\n",
      "✓ Processed 30 pages from BESTBUY_2024Q2_10Q.pdf in 4.69s | Image: 14.20MB → Base64: 18.94MB (ratio: 1.33x)\n",
      "✓ Processed 188 pages from MGMRESORTS_2018_10K.pdf in 28.93s | Image: 102.54MB → Base64: 136.71MB (ratio: 1.33x)\n",
      "✓ Processed 136 pages from LOCKHEEDMARTIN_2021_10K.pdf in 19.90s | Image: 63.93MB → Base64: 85.24MB (ratio: 1.33x)\n",
      "✓ Processed 124 pages from VERIZON_2022_10K.pdf in 18.61s | Image: 86.01MB → Base64: 114.69MB (ratio: 1.33x)\n",
      "✓ Processed 392 pages from CVSHEALTH_2018_10K.pdf in 56.44s | Image: 223.44MB → Base64: 297.92MB (ratio: 1.33x)\n",
      "\n",
      "============================================================\n",
      "Processing Summary:\n",
      "  Total PDFs: 74\n",
      "  Successfully processed: 72\n",
      "  Failed: 2\n",
      "  Total pages extracted: 10955\n",
      "  Total image size: 5616.41 MB\n",
      "  Total base64 size: 7488.57 MB\n",
      "  Overall compression ratio: 1.33x\n",
      "============================================================\n",
      "\n",
      "Failed to open:\n",
      "  - KRAFTHEINZ_2019_10K.pdf\n",
      "  - AMD_2015_10K.pdf\n"
     ]
    }
   ],
   "source": [
    "results = process_pdfs_to_base64(\n",
    "    pdf_folder=PDF_FOLDER,\n",
    "    dpi=DPI,\n",
    "    fmt=FORMAT,\n",
    "    max_workers=MAX_WORKERS,\n",
    "    number_pdfs=None  # Set to a number to limit processing, or None for all\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47df0bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved base64 encodings to pdf_pages_base64.csv\n"
     ]
    }
   ],
   "source": [
    "save_results_to_csv(results, output_file=OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17713706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10955 entries, 0 to 10954\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   pdf_filename       10955 non-null  object\n",
      " 1   pdf_stem           10955 non-null  object\n",
      " 2   page_number        10955 non-null  int64 \n",
      " 3   image_size_bytes   10955 non-null  int64 \n",
      " 4   base64_size_bytes  10955 non-null  int64 \n",
      " 5   base64_encoding    10955 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 513.6+ KB\n"
     ]
    }
   ],
   "source": [
    "pdf_pages_df = pd.read_csv(\"pdf_pages_base64.csv\")\n",
    "pdf_pages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7326b3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "152.15277777777777\n"
     ]
    }
   ],
   "source": [
    "print(pdf_pages_df[\"pdf_filename\"].nunique())\n",
    "print(len(pdf_pages_df) / pdf_pages_df[\"pdf_filename\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4235f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7488.567134857178\n"
     ]
    }
   ],
   "source": [
    "print(pdf_pages_df[\"base64_size_bytes\"].sum() / 1024 / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58f51afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6835235052487448"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7488 / 10955"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d1bc4",
   "metadata": {},
   "source": [
    "# FinanceBench Recall per PDF test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbcb83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_filename\n",
      "3M_2018_10K.pdf                       160\n",
      "3M_2022_10K.pdf                       252\n",
      "3M_2023Q2_10Q.pdf                      92\n",
      "ACTIVISIONBLIZZARD_2019_10K.pdf       198\n",
      "AES_2022_10K.pdf                      257\n",
      "AMAZON_2017_10K.pdf                    85\n",
      "AMAZON_2019_10K.pdf                    83\n",
      "AMCOR_2020_10K.pdf                    195\n",
      "AMCOR_2022_8K_dated-2022-07-01.pdf      9\n",
      "AMCOR_2023Q2_10Q.pdf                   57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group the records by pdf_filename\n",
    "grouped_pdfs = pdf_pages_df.groupby(\"pdf_filename\")\n",
    "# Now `grouped` is a DataFrameGroupBy object, and you can iterate or aggregate as needed.\n",
    "# For example, to see how many pages per PDF:\n",
    "page_counts = grouped_pdfs.size()\n",
    "print(page_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782a3d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.',\n",
       " 'doc_name': '3M_2018_10K',\n",
       " 'answer': '$1577.00',\n",
       " 'evidence_page_num': 59}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = load_dataset(\"PatronusAI/financebench\")[\"train\"]\n",
    "\n",
    "filtered_questions = []\n",
    "for q in questions:\n",
    "    # evidence_page_num is inside first evidence dict, if it exists\n",
    "    evidence_page_num = None\n",
    "    if q.get(\"evidence\") and isinstance(q[\"evidence\"], list) and len(q[\"evidence\"]) > 0:\n",
    "        evidence_page_num = q[\"evidence\"][0].get(\"evidence_page_num\")\n",
    "    filtered_questions.append({\n",
    "        \"question\": q.get(\"question\"),\n",
    "        \"doc_name\": q.get(\"doc_name\"),\n",
    "        \"answer\": q.get(\"answer\"),\n",
    "        \"evidence_page_num\": evidence_page_num\n",
    "    })\n",
    "\n",
    "filtered_questions[0]  # show an example of filtered row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ecb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import AnyHttpUrl\n",
    "import weaviate\n",
    "from weaviate.collections.classes.config import Configure, DataType, Property\n",
    "from weaviate.config import AdditionalConfig, Timeout\n",
    "\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    weaviate_url=os.getenv(\"WEAVIATE_URL\"),\n",
    "    auth_credentials=os.getenv(\"WEAVIATE_API_KEY\"),\n",
    "    additional_config=AdditionalConfig(timeout=Timeout(insert=1800)),\n",
    ")\n",
    "\n",
    "collection_name = \"FinanceBenchPDF\"\n",
    "\n",
    "def drop_and_create_collection(collection_name):\n",
    "    if weaviate_client.collections.exists(collection_name):\n",
    "        weaviate_client.collections.delete(collection_name)\n",
    "\n",
    "    collection = weaviate_client.collections.create(\n",
    "        name=collection_name,\n",
    "        properties=[\n",
    "            Property(name=\"page_number\", data_type=DataType.INT, skip_vectorization=True),\n",
    "            Property(name=\"page_image\", data_type=DataType.BLOB),\n",
    "        ],\n",
    "        vector_config=Configure.MultiVectors.multi2vec_weaviate(\n",
    "            base_url=AnyHttpUrl(\"https://dev-embedding.labs.weaviate.io\"),\n",
    "            image_fields=[\"page_image\"],\n",
    "            model=\"ModernVBERT/colmodernvbert\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PDFwithPageNum(BaseModel):\n",
    "    page_number: int\n",
    "    page_image: str\n",
    "\n",
    "def upload_pdfs_to_weaviate(\n",
    "    weaviate_collection,\n",
    "    pdf_with_page_num\n",
    "):\n",
    "    with weaviate_collection.batch.fixed_size(20) as batch:\n",
    "        for page in pdf_with_page_num:\n",
    "            batch.add_object(\n",
    "                properties={\n",
    "                    \"page_number\": int(page.page_number),\n",
    "                    \"page_image\": page.page_image\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d561308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.',\n",
       " 'doc_name': '3M_2018_10K',\n",
       " 'answer': '$1577.00',\n",
       " 'evidence_page_num': 59}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c23151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='[[ ## re...: None}, annotations=[]), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/gpt-4.1-mini\",\n",
    "    cache=False,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "class AssessAnswerAlignment(dspy.Signature):\n",
    "    \"\"\"Assess the alignment of a system answer with the ground truth answer.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(description=\"The question to be answered.\")\n",
    "    ground_truth_answer: str = dspy.InputField(description=\"The correct answer to the question.\")\n",
    "    system_answer: str = dspy.InputField(description=\"The answer provided by the system.\")\n",
    "    alignment_score: bool = dspy.OutputField(description=\"True if the system answer is aligned with the ground truth answer, False otherwise.\")\n",
    "\n",
    "llm_as_judge = dspy.ChainOfThought(AssessAnswerAlignment)\n",
    "\n",
    "question = \"What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\"\n",
    "ground_truth_answer = \"$1577.00\"\n",
    "system_answer_1 = \"The capital expenditure amount for 3M in FY2018 was $1,577 million.\"\n",
    "system_answer_2 = \"3M's capital expenditure for 2018 was $530 million.\"\n",
    "\n",
    "print(llm_as_judge(question=question, ground_truth_answer=ground_truth_answer, system_answer=system_answer_1).alignment_score)\n",
    "print(llm_as_judge(question=question, ground_truth_answer=ground_truth_answer, system_answer=system_answer_2).alignment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14745767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.agents.query import QueryAgent\n",
    "\n",
    "qa = QueryAgent(\n",
    "    client=weaviate_client,\n",
    "    collections=[\"FinanceBenchPDF\"],\n",
    "    agents_host=\"https://dev-agents.labs.weaviate.io\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351365ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF filename: 3M_2018_10K.pdf\n",
      "Page numbers in this PDF: 160\n",
      "Uploaded 160 pages from 3M_2018_10K.pdf to Weaviate in 138.66 seconds.\n",
      "Query: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "Looking for page 59.\n",
      "--------------------\n",
      "Retrieved page numbers: [59, 39, 26, 43, 42, 7, 15, 106, 49, 41, 58, 31, 44, 74, 61, 47, 126, 75, 14, 78]\n",
      "--------------------\n",
      "QA Agent Response:\n",
      " The FY2018 capital expenditure amount (purchases of property, plant, and equipment) for 3M is $1,577 million.\n",
      "Alignment score: True\n",
      "==== Aggregate Metrics ====\n",
      "Recall@1: 1/1 = 1.000\n",
      "Recall@5: 1/1 = 1.000\n",
      "Recall@20: 1/1 = 1.000\n",
      "Avg. Alignment Score: 1.000\n",
      "Alignment Scores Breakdown: [True]\n",
      "Average upload time: 138.66 seconds.\n",
      "Max upload time: 138.66 seconds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics lists\n",
    "recall_at_1_list = []\n",
    "recall_at_5_list = []\n",
    "recall_at_20_list = []\n",
    "alignment_scores = []\n",
    "\n",
    "upload_times = []\n",
    "\n",
    "# Loop through each PDF and its group of pages\n",
    "for pdf_filename, group in grouped_pdfs:\n",
    "    print(f\"Processing PDF filename: {pdf_filename}\")\n",
    "    print(\"Page numbers in this PDF:\", len(list(group[\"page_number\"])))\n",
    "\n",
    "    # Convert each row in group to a PDFwithPageNum instance\n",
    "    pdf_with_page_num = [\n",
    "        PDFwithPageNum(\n",
    "            page_number=row[\"page_number\"],\n",
    "            page_image=row[\"base64_encoding\"]\n",
    "        )\n",
    "        for _, row in group.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Drop and create (reset) the collection in Weaviate\n",
    "    weaviate_collection = drop_and_create_collection(collection_name)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    # Upload all pages of this PDF to Weaviate\n",
    "    upload_pdfs_to_weaviate(weaviate_collection, pdf_with_page_num)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    upload_times.append(elapsed_time)\n",
    "    print(f\"Uploaded {len(pdf_with_page_num)} pages from {pdf_filename} to Weaviate in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    lookup_filename = pdf_filename.split(\".\")[0]\n",
    "    question_row = next((q for q in filtered_questions if q['doc_name'] == lookup_filename), None)\n",
    "\n",
    "    if question_row is None:\n",
    "        print(f\"No question found for {lookup_filename}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    query = question_row[\"question\"]\n",
    "    ground_truth_page_num = question_row[\"evidence_page_num\"]\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Looking for page {ground_truth_page_num}.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "    results = weaviate_collection.query.near_text(\n",
    "        query=query,\n",
    "        return_metadata=[\"distance\"],\n",
    "        return_properties=[\"page_number\"],\n",
    "        limit=20\n",
    "    )\n",
    "\n",
    "    retrieved_page_numbers = [obj.properties[\"page_number\"] for obj in results.objects]\n",
    "    print(\"Retrieved page numbers:\", retrieved_page_numbers)\n",
    "    print(\"-\"*20)\n",
    "\n",
    "    # Calculate recall@1, @5, @20\n",
    "    recall_at_1 = int(ground_truth_page_num in retrieved_page_numbers[:1])\n",
    "    recall_at_5 = int(ground_truth_page_num in retrieved_page_numbers[:5])\n",
    "    recall_at_20 = int(ground_truth_page_num in retrieved_page_numbers[:20])\n",
    "\n",
    "    recall_at_1_list.append(recall_at_1)\n",
    "    recall_at_5_list.append(recall_at_5)\n",
    "    recall_at_20_list.append(recall_at_20)\n",
    "\n",
    "    # Now get the answer from the agent\n",
    "    response = qa.ask(query)\n",
    "    print(\"QA Agent Response:\\n\", response.final_answer)\n",
    "\n",
    "    ground_truth_answer = question_row[\"answer\"]\n",
    "    system_answer = response.final_answer\n",
    "\n",
    "    alignment_score = llm_as_judge(\n",
    "        question=query,\n",
    "        ground_truth_answer=ground_truth_answer,\n",
    "        system_answer=system_answer\n",
    "    ).alignment_score\n",
    "\n",
    "    alignment_scores.append(alignment_score)\n",
    "    print(f\"Alignment score: {alignment_score}\")\n",
    "\n",
    "    break\n",
    "\n",
    "# Aggregate and print overall metrics\n",
    "print(\"==== Aggregate Metrics ====\")\n",
    "print(f\"Recall@1: {sum(recall_at_1_list)}/{len(recall_at_1_list)} = {sum(recall_at_1_list)/max(1,len(recall_at_1_list)):.3f}\")\n",
    "print(f\"Recall@5: {sum(recall_at_5_list)}/{len(recall_at_5_list)} = {sum(recall_at_5_list)/max(1,len(recall_at_5_list)):.3f}\")\n",
    "print(f\"Recall@20: {sum(recall_at_20_list)}/{len(recall_at_20_list)} = {sum(recall_at_20_list)/max(1,len(recall_at_20_list)):.3f}\")\n",
    "\n",
    "alignment_avg = sum(map(int, alignment_scores))/len(alignment_scores)\n",
    "print(f\"Avg. Alignment Score: {alignment_avg:.3f}\")\n",
    "print(f\"Alignment Scores Breakdown: {alignment_scores}\")\n",
    "\n",
    "print(f\"Average upload time: {sum(upload_times)/len(upload_times):.2f} seconds.\")\n",
    "print(f\"Max upload time: {max(upload_times):.2f} seconds.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba2029",
   "metadata": {},
   "source": [
    "### The following results are from completing 35 PDFs with the script above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c738bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Aggregate Metrics ====\n",
      "Recall@1: 3/35 = 0.086\n",
      "Recall@5: 6/35 = 0.171\n",
      "Recall@20: 13/35 = 0.371\n",
      "Avg. Alignment Score: 0.629\n",
      "Alignment Scores Breakdown: [True, False, False, True, True, False, True, True, True, False, True, False, True, False, True, True, True, False, False, False, True, True, False, True, False, False, True, True, False, True, True, True, True, True, True]\n",
      "Average upload time: 172.38 seconds.\n",
      "Max upload time: 2038.15 seconds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate and print overall metrics\n",
    "print(\"==== Aggregate Metrics ====\")\n",
    "print(f\"Recall@1: {sum(recall_at_1_list)}/{len(recall_at_1_list)} = {sum(recall_at_1_list)/max(1,len(recall_at_1_list)):.3f}\")\n",
    "print(f\"Recall@5: {sum(recall_at_5_list)}/{len(recall_at_5_list)} = {sum(recall_at_5_list)/max(1,len(recall_at_5_list)):.3f}\")\n",
    "print(f\"Recall@20: {sum(recall_at_20_list)}/{len(recall_at_20_list)} = {sum(recall_at_20_list)/max(1,len(recall_at_20_list)):.3f}\")\n",
    "\n",
    "alignment_avg = sum(map(int, alignment_scores))/len(alignment_scores)\n",
    "print(f\"Avg. Alignment Score: {alignment_avg:.3f}\")\n",
    "print(f\"Alignment Scores Breakdown: {alignment_scores}\")\n",
    "\n",
    "print(f\"Average upload time: {sum(upload_times)/len(upload_times):.2f} seconds.\")\n",
    "print(f\"Max upload time: {max(upload_times):.2f} seconds.\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
