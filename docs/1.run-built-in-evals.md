# 1. Run Evals

```python
import query_agent_benchmarker

# reads parameters from default config path

query_agent_benchmarker.run_eval()

# custom config path

query_agent_benchmarker.run_eval(
	config="./my-config.yml"
)

# pass in override params

query_agent_benchmarker.run_eval(
	dataset_name="bright/psychology",
    agent="query-agent-search-mode"
)
```